\chapter{Current Limitations of Language Models}

\section{The Softmax Bottleneck}

The majority of parametric LMs use a Softmax function operating on the context \emph{c}, and a word embedding $ e_w $ to define the conditional distribution $ P_\theta(w|c) $, where \emph{w} stands for a word, and $ \theta $ are the parameters of the model. More specifically, the model distribution is usually written as:

\begin{displaymath}
    P_\theta(w | c) = \frac{h^Te_w}{\sum_{e_{w'}} h^Te_{w'}}
\end{displaymath}

Here, \emph{h} is a function of \emph{c} and it is usually obtained using a Recurrent Neural Network, and $ e_w $ is the embedding for word \emph{w}. Let $ h \in R^D $ and $ e_w \in R^D $ and let both \emph{h} and $ e_w $ depend on $ \theta $. We refer to the dot product $ h^Te_w $ as a \emph{logit}.

Now even though in general, one might argue that natural languages contain an infinite amount of contexts, let's take a look at the finite case first. We can assume that a natural language is consisted of \emph{N} contexts, and \emph{M} words. Given this, we can describe Language Modelling as a matrix factorization problem. Consider the following matrices:

\begin{displaymath}
    \begin{matrix}
        H_\theta &= \begin{bmatrix}
               h^T_{c_1} \\
               h^T_{c_2} \\
               \vdots \\
               h^T_{c_N}
              \end{bmatrix}
        \ \ \      
        E_\theta &= \begin{bmatrix}
           e^T_{w_1} \\
           e^T_{w_2} \\
           \vdots \\
           e^T_{w_M}
          \end{bmatrix}
    \end{matrix} \\
\end{displaymath}
\begin{displaymath}
    \begin{matrix}
    A &= \begin{bmatrix}
       \log P(w_1 | c_1) & \log P(w_2 | c_1) & \hdots & \log P(w_M | c_1)  \\
       \log P(w_1 | c_2) & \log P(w_2 | c_2) & \hdots & \log P(w_M | c_2) \\
       \vdots & \vdots & \ddots & \vdots \\
       \log P(w_1 | c_N) & \log P(w_1 | c_N) & \hdots & \log P(w_M | c_N)
      \end{bmatrix}
    \end{matrix}
\end{displaymath}

Let, $ H_\theta \in R^{N \times D} $ be a matrix containing the hidden states for every context. Let, $ E_\theta \in R^{M \times D}$ be a matrix containing the word embeddings for every word. Let, $ A $ be a matrix containing the true log probabilities of every word, given every context. Then, language modelling can be described as:

\begin{displaymath}
    H_\theta E^T_\theta = \hat A
\end{displaymath}

Where, we want $ \hat A $ to be as close as possible to the true $ A $. Now we can ask the following question:

\begin{center}
    \emph{What is the expressiveness of $ \hat{A} $}
\end{center}






\section{Single Transformation}

Another limitation of classical language models is the output projection layer. Taking the matrix-vector product between the embedding matrix (nowadays weights between the output projection and the embedding matrix are usually tied) and the hidden state, further exponentiating and normalizing the result to get a probability distribution, essentially results in a transformation that behaves as a single mode Gaussian around the hidden state. This means that the majority of the probability mass is concentrated on one very small continuous subspace of the embedding space. This subspace usually corresponds to the surroundings of the word embedding that is mostly associated with the given context throughout training. Take the following sentence as an example:

\begin{center}
    \emph{“I want to buy…”}
\end{center}

It is easy to see that multiple words are likely to come after this particular context even though they might be associated with different contexts on average. This is a very common scenario in natural languages where the distribution over the vocabulary is considered to be a \emph{fat tail} distribution. For example take the words \emph{car} and \emph{computer}. By training a Language Model and plotting the embeddings, it is possible to visualize that the neighbors of \emph{car} in the embedding space are \emph{truck, auto, bus, vehicle} etc. and the neighbors of \emph{computer} are \emph{desktop, portable, electronic, laptop} etc. Refer to figures \ref{car} and \ref{computer}. This indicates that \emph{car} and \emph{computer} are in different neighborhoods of the embedding space.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/computer_emb_neighbors.png}
%   \caption{A subfigure}
  \label{car:neighborhood}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/computer_nearest_neighbors.png}
%   \caption{A subfigure}
  \label{car:neighbor}
\end{subfigure}
\caption{Neighborhood of \emph{car} in the embedding space of AWD-LSTM.}
\label{car}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/car_emb_neighbor.png}
%   \caption{A subfigure}
  \label{computer:neighborhood}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/car_nearest_neighbors.png}
%   \caption{A subfigure}
  \label{computer:neighbor}
\end{subfigure}
\caption{Neighborhood of \emph{computer} in the embedding space of AWD-LSTM.}
\label{computer}
\end{figure}

Now imagine every single word that can finish the previous sentence. Every one of those words is embedded into its own neighborhood where it is close to other words that share the same context on average. Therefore, a simple single mode Gaussian seems very limited for all those \emph{"fat tail"} situations in natural languages. Ideally, we would like something that can adapt based on the context, i.e. given a hidden state we can obtain different type of distributions. It is also possible that the success of MoS \cite{yang2017breaking} and DoC \cite{takase2018direct} papers is due to the fact that they learn a dynamic mixture of Gaussians, which by definition is more expressive. Additionally, based on the amount of Gaussians in the mixture, those approaches are more or less capable of solving the previously mentioned issues.

To generalize, it seems unlikely that a single general distribution, parameterized by the hidden state, can capture all possible situations in Language Modelling. Therefore, it seems reasonable that based on the context, or the hidden state if you want, we would like to obtain custom distributions that specifically fit the need of the current context. But what if we can start with a "simple", single, general distribution and then based on the context, distort it into a more complex distribution if needed? That would alleviate the need to manually tune the amount of components in the mixture and can be seen as a generalization of MoS \cite{yang2017breaking} and DoC \cite{takase2018direct}.

At the end of this thesis I propose a concept that should be able to solve this problem in theory.
