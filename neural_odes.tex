\chapter{Neural Ordinary Differential Equations}

In recent years Residual Networks \cite{he2016deep} have brought a great success in Deep Learning. They have proven to be effective against the vanishing gradient problem and have therefore drastically eased the optimization of very deep neural networks. If we refer to the output vector of each layer as a \emph{"hidden state"} and denote it with $h_t$ where \emph{t} stands for the layer, then Residual Networks can be mathematically described as:

\begin{displaymath}
    h_{t+1} = h_{t} + f(h_{t}, \theta_{t})
\end{displaymath}

where $ t \in \{0, ..., T\} $, $ h_t \in R^D $ and $ \theta_t $ being the parameters of the \emph{t}-th layer. These iterative updates can be seen as an Euler discretization of a continuous transformation \cite{lu2017beyond}\cite{haber2017stable}\cite{ruthotto2018deep}.

What happens as we add more layers and take smaller steps? In the limit, we parameterize the continuous dynamics of the hidden state using an ordinary differential equation (ODE) specified by a neural network:

\begin{equation}
    \label{test}
    \frac{d h(t)}{d t} = f(h(t), t, \theta )
\end{equation}

A recent best paper award winner at NeurIPS \cite{chen2018neural} introduced this as a new family of deep neural network models, where the neural network simply outputs the gradient of the hidden state with respect to depth. Then given an initial state and the differential equation parameterized by the neural network, they obtain the final state by solving an ODE. The analogy they make is the one that considers Neural ODEs to be the continuous case of residual networks. This means that the amount of layers is dynamically decided by the ODE solver.
In equation \ref{test} \emph{h(t)} is the hidden state in the \emph{t}-th layer, 


