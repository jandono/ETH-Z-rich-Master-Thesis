\chapter{Introduction}

Anything we see or do, can be described and contained within a sequence of words, meaning that the entire complexity of the world can be embedded in a piece of text. This is exactly what makes text and textual communication so important in our daily lives. Additionally, this is also what makes text processing and textual communication so complex for machines. Namely, the area of Computer Science that bothers with these problems is called Natural Language Processing (NLP). NLP is a vast area with many subcategories, but without doubt, one of its core and most difficult problems is text understanding and text generation.

In NLP, the tools that are used for text generation are called Language Models (LM). Let's consider the following sentence:

\begin{center}
    \emph{Look at all those clouds, it is going to ...}
\end{center}

Given this sentence as a context, a Language Model will then try estimate what is the most likely word that can end this sentence. From a mathematical point of view, LMs are trying to learn a context conditioned probability distribution over a vocabulary. This means that given a vocabulary of available words and a sequence of words representing the history or the context, a Language Model will process the context and return a discrete probability distribution over the vocabulary. 

\begin{displaymath}
    P(w | w_{1..i-1})
\end{displaymath}

Here $w_{1..i-1}$, is the context, often denoted as simply $h$, and $w$ is a discrete random variable that reperesents the vocabulary. We can then proceed with generating the next word by simply selecting the word with highest probability.

\begin{displaymath}
    \hat{w} = argmax_w P(w | w_{1..i-1})
\end{displaymath}

However, very often we don't want to simply generate a single word given a context, but instead want to generate whole sequences. Therefore, the LMs can also be seen as tools that model the joint probability distribution over a textual sequence. Or mathematically speaking:

\begin{displaymath}
    P(w_1, ..., w_n) = \prod_i^n P(w_i | w_{1..i-1})
\end{displaymath}

First LMs were count based and called N-grams \cite{martin2009speech}. However, with the recent advances in Deep Learning, LMs based on Neural Networks are currently dominating the field. Since the first Neural Language Model \cite{bengio2003neural} which was based on Feedforward Neural Networks, things have evolved and now Recurrent Neural Networks (RNN) \cite{mikolov2010recurrent} are the standard. Additionally, as neural networks are trained with gradient based methods and back-propagation \cite{rumelhart1988learning}, people have figured out that RNNs, when processing long contexts can suffer from the vanishing or exploding gradients problem \cite{hochreiter1998vanishing}\cite{pascanu2012understanding}\cite{pascanu2013difficulty}. Because of this Vanilla RNNs were substituted with Long short-term Memory (LSTM) \cite{hochreiter1997long} RNNs, which for the past few years has been a goto method for commercial Language Modelling.

In this thesis, I will first try to give an overview of the current limitations of Language Modelling. I will then proceed and describe how previous work have tried to solve these limitations. Then I will introduce a novel idea for overcoming the limitations of Language Modelling. Finally, I will describe the neural architecture of my models, as well as present the results from my experiments.
